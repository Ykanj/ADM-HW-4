{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Bonus Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ask to chatGPT which are the three algorithms for Density-Based Clustering, it suggest to me:\n",
    "\n",
    "- **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise): DBSCAN is a density-based clustering non-parametric algorithm. It groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions1.\n",
    "\n",
    "- **OPTICS** (Ordering Points To Identify the Clustering Structure): OPTICS is an algorithm for finding density-based clusters in spatial data. It’s similar to DBSCAN, but it addresses one of DBSCAN’s major weaknesses: the problem of detecting meaningful clusters in data of varying density2.\n",
    "\n",
    "- **Mean Shift Clustering**: Mean shift is a non-parametric feature-space analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm. It is particularly useful for datasets where the clusters have arbitrary shapes and are not well-separated by linear boundaries3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm starting thinking that the **DBSCAN** is the best choose, because it not require to specify the number of clusters in the data a priori, It can even find a cluster completely surrounded by (but not connected to) a different cluster and the parameters required can be set by a domain expert, if the data is well understood, but given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions.\n",
    "Instead, **OPTICS** is similar to the previous, but it addresses the problem of detecting meaningful clusters in data of varying density; to do so, the points of the database are (linearly) ordered such that spatially closest points become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that must be accepted for a cluster so that both points belong to the same cluster. This is represented as a dendrogram.\n",
    "So i think that the last algorithm can explain and rappresent better our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('vodclickstream_uk_movies_03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 671736 entries, 0 to 671735\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   Unnamed: 0    671736 non-null  int64  \n",
      " 1   datetime      671736 non-null  object \n",
      " 2   duration      671736 non-null  float64\n",
      " 3   title         671736 non-null  object \n",
      " 4   genres        671736 non-null  object \n",
      " 5   release_date  671736 non-null  object \n",
      " 6   movie_id      671736 non-null  object \n",
      " 7   user_id       671736 non-null  object \n",
      "dtypes: float64(1), int64(1), object(6)\n",
      "memory usage: 41.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to use sklearn library that consider that OPTICS is related to DBSCAN so this algorithm keeps cluster hierarchy for a variable neighborhood radius. Clusters are then extracted using a DBSCAN-like method (cluster_method = ‘dbscan’) or an automatic technique(cluster_method = ‘xi’).\n",
    "\n",
    "This implementation deviates from the original OPTICS by first performing k-nearest-neighborhood searches on all points to identify core sizes, then computing only the distances to unprocessed points when constructing the cluster order. Note that we do not employ a heap to manage the expansion candidates, so the time complexity will be O(n^2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from sklearn.cluster import OPTICS, cluster_optics_dbscan\n",
    "from sklearn.preprocessing import normalize, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 671736 entries, 0 to 671735\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   Unnamed: 0    671736 non-null  int64         \n",
      " 1   datetime      671736 non-null  datetime64[ns]\n",
      " 2   duration      671736 non-null  float64       \n",
      " 3   title         671736 non-null  object        \n",
      " 4   genres        671736 non-null  object        \n",
      " 5   release_date  641432 non-null  datetime64[ns]\n",
      " 6   movie_id      671736 non-null  object        \n",
      " 7   user_id       671736 non-null  object        \n",
      "dtypes: datetime64[ns](2), float64(1), int64(1), object(4)\n",
      "memory usage: 41.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.datetime = pd.to_datetime(df.datetime)\n",
    "df.release_date = pd.to_datetime(df.release_date, errors='coerce')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic Question\n",
    "\n",
    "Federico studies in a demanding university where he has to take a certain number $N$ of exams to graduate, but he is free to choose in which order he will take these exams. Federico is panicking since this university is not only one of the toughest in the world but also one of the weirdest. His final grade won't depend at all on the mark he gets in these courses: there's a precise evaluation system.\n",
    "\n",
    "He was given an initial personal score of $S$ when he enrolled, which changes every time he takes an exam: now comes the crazy part. He soon discovered that every of the $N$ exams he has to take is assigned a mark $p$. Once he has chosen an exam, his score becomes equal to the mark $p$, and at the same time, the scoring system changes:\n",
    "\n",
    "If he takes an \"easy\" exam (the score of the exam being less than his score), every other exam's mark is increased by the quantity $S-p$.\n",
    "If he takes a \"hard\" exam (the score of the exam is greater than his score), every other exam's mark is decreased by the quantity $p-S$.\n",
    "\n",
    "In this chaotic university where the only real exam seems to be choosing the best way to take exams, you are the poor student advisor who is facing a long queue of confused people who need some help. Federico is next in line, and he comes up in turn with an inescapable question: he wants to know which is the highest score possible he could get.\n",
    "\n",
    "a) Fortunately, you have a computer app designed by a brilliant student. Federico wants you to show him the code which this app is based on because he wants to do paid counseling for other desperate students: in a recursive fashion, the helped helps the helpable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the highest score that can be achieved in this particular university, we have decided to create a recursive algorithm that performs the necessary operations to update the score at each iteration and then repeats them until it finds the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1: 11\n",
      "Output 2: 44\n",
      "Output 3: 109\n"
     ]
    }
   ],
   "source": [
    "def highest_possible_score(score, marks):\n",
    "    if not marks:  # If there are no exams left to take, return the current score\n",
    "        return score\n",
    "\n",
    "    max_score = 0  # Initialize max_score\n",
    "\n",
    "    for i, exam_score in enumerate(marks):\n",
    "        new_marks = marks[:i] + marks[i + 1:]  # Exclude the current exam score\n",
    "        new_score = exam_score  # Federico's score after taking the exam\n",
    "\n",
    "        # Determine the change in marks based on the exam difficulty\n",
    "        if exam_score < score:  # If it's an \"easy\" exam\n",
    "            new_marks = [mark + (score - exam_score) if mark + (score - exam_score)>=0 else 0 for mark in new_marks]\n",
    "        elif exam_score > score:  # If it's a \"hard\" exam\n",
    "            new_marks = [mark - (exam_score - score) if mark - (exam_score - score)>=0 else 0 for mark in new_marks]\n",
    "\n",
    "        # Recursively calculate the highest possible score after taking this exam\n",
    "        max_score = max(max_score, highest_possible_score(new_score, new_marks))\n",
    "\n",
    "    return max_score\n",
    "\n",
    "# Test cases\n",
    "s1 = 8\n",
    "p1 = [5, 7, 1]\n",
    "result1 = highest_possible_score(s1, p1)\n",
    "print(\"Output 1:\", result1)\n",
    "\n",
    "s2 = 25\n",
    "p2 = [18, 24, 21, 32, 27]\n",
    "result2 = highest_possible_score(s2, p2)\n",
    "print(\"Output 2:\", result2)\n",
    "\n",
    "s3 = 30\n",
    "p3 = [13, 27, 41, 59, 28, 33, 39, 19, 52, 48, 55, 79]\n",
    "result3 = highest_possible_score(s3, p3)\n",
    "print(\"Output 3:\", result3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational complexity\n",
    "\n",
    "\n",
    "b) Federico is getting angry because he claims that your code is slow! Show him formally with a big-O notation that he is as crazy as this university!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complexity of my algorithm is $O(n! \\cdot n^2)$.\n",
    "\n",
    "The number of recursive calls equals the number of permutations of possible exams, which is n!, where n is the number of exams.\n",
    "Each recursive call executes a for loop over all the remaining exams.\n",
    "Within the loop, operations involve copying the list of exams (O(n)), comparisons (O(1)), and addition or subtraction operations on the elements of the list (O(n)).\n",
    "\n",
    "Therefore:\n",
    "\n",
    "- O(n!) arises from the number of permutations of possible exams.\n",
    "- $O(n^2)$ arises from the operations within each recursive call.\n",
    "\n",
    "Indeed, as n grows larger, the execution time increases. In the case of the third example where p has a length of 12, the algorithm takes approximately 8 minutes to run. When considering comparing all the exams in a study program, it might take far too long, especially considering that a three-year study program has around 25 exams.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "c) If, unfortunately, Federico is right in the grip of madness, he will threaten you to optimize the code through a different approach. You should end this theater of the absurd by any means! (And again, formally prove that you improved time complexity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to optimize it by creating a tuple to store each unique combination of score and marks, to avoiding redundant operations and with memoization, the number of unique subproblems calculated reduces significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1: 11\n",
      "Output 2: 44\n",
      "Output 3: 109\n"
     ]
    }
   ],
   "source": [
    "def highest_possible_score(score, marks, memo={}):\n",
    "    if not marks:\n",
    "        return score\n",
    "\n",
    "    if (score, tuple(marks)) in memo:  # If result is already calculated, return it\n",
    "        return memo[(score, tuple(marks))]\n",
    "\n",
    "    max_score = 0\n",
    "\n",
    "    for i, exam_score in enumerate(marks):\n",
    "        new_marks = marks[:i] + marks[i + 1:]\n",
    "\n",
    "        if exam_score < score:\n",
    "            new_marks = [mark + (score - exam_score) if mark + (score - exam_score)>=0 else 0 for mark in new_marks]\n",
    "        elif exam_score > score:\n",
    "            new_marks = [mark - (exam_score - score) if mark - (exam_score - score)>=0 else 0 for mark in new_marks]\n",
    "\n",
    "        new_score = exam_score\n",
    "        new_max_score = highest_possible_score(new_score, new_marks, memo)\n",
    "        max_score = max(max_score, new_max_score)\n",
    "\n",
    "    memo[(score, tuple(marks))] = max_score  # Store the calculated result in the memo\n",
    "\n",
    "    return max_score\n",
    "\n",
    "s1 = 8\n",
    "p1 = [5, 7, 1]\n",
    "result1 = highest_possible_score(s1, p1)\n",
    "print(\"Output 1:\", result1)\n",
    "\n",
    "s2 = 25\n",
    "p2 = [18, 24, 21, 32, 27]\n",
    "result2 = highest_possible_score(s2, p2)\n",
    "print(\"Output 2:\", result2)\n",
    "\n",
    "s3 = 30\n",
    "p3 = [13, 27, 41, 59, 28, 33, 39, 19, 52, 48, 55, 79]\n",
    "result3 = highest_possible_score(s3, p3)\n",
    "print(\"Output 3:\", result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memoization effectively limits the number of unique subproblems to be solved to a maximum of the number of possible distinct combinations of score and marks.\n",
    "The recursion in the worst case, for each exam, will consider two possibilities (easy or hard). So, if there are n exams, it would explore $2^n$ possibilities.\n",
    "\n",
    "Considering these points, the time complexity of the memoized highest_possible_score function can be seen as improved from the naive exponential time complexity to something closer to $O(n \\cdot s)$, where n is the number of exams and s is the range of possible score differences between exams.\n",
    "\n",
    "It is evidently faster, in fact it take only 0.4 second to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The optimization of chat GPT\n",
    "\n",
    "d) Ask chatGPT for a third (optimized) implementation and analyze again its time complexity. Be careful (and crafty) in defining the prompt, and challenge the machine in this coding question!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I send to chat Gpt my last code and i ask it to optimize my code trying to reduce the computational complexity.  this is it output: \n",
    "\n",
    "In this code, I added a memo parameter to the highest_possible_score function that is used to store intermediate results. This memo dictionary is passed to each recursive call of the function, thus allowing it to store the results of each sub-problem. When the function is called with the same score and marks parameters, it simply returns the stored result instead of calculating it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1: 11\n",
      "Output 2: 44\n",
      "Output 3: 109\n"
     ]
    }
   ],
   "source": [
    "def highest_possible_score(score, marks, memo=None):\n",
    "    if memo is None:\n",
    "        memo = {}\n",
    "\n",
    "    if not marks:\n",
    "        return score\n",
    "\n",
    "    if (score, tuple(marks)) in memo:  # If result is already calculated, return it\n",
    "        return memo[(score, tuple(marks))]\n",
    "\n",
    "    max_score = 0\n",
    "\n",
    "    for i, exam_score in enumerate(marks):\n",
    "        new_marks = marks[:i] + marks[i + 1:]\n",
    "\n",
    "        if exam_score < score:\n",
    "            new_marks = [mark + (score - exam_score)  if mark + (score - exam_score)>=0 else 0  for mark in new_marks]\n",
    "        elif exam_score > score:\n",
    "            new_marks = [mark - (exam_score - score) if mark - (exam_score - score)>=0 else 0 for mark in new_marks]\n",
    "\n",
    "        new_score = exam_score\n",
    "        new_max_score = highest_possible_score(new_score, new_marks, memo)\n",
    "        max_score = max(max_score, new_max_score)\n",
    "\n",
    "    memo[(score, tuple(marks))] = max_score  # Store the calculated result in the memo\n",
    "\n",
    "    return max_score\n",
    "\n",
    "s1 = 8\n",
    "p1 = [5, 7, 1]\n",
    "result1 = highest_possible_score(s1, p1)\n",
    "print(\"Output 1:\", result1)\n",
    "\n",
    "s2 = 25\n",
    "p2 = [18, 24, 21, 32, 27]\n",
    "result2 = highest_possible_score(s2, p2)\n",
    "print(\"Output 2:\", result2)\n",
    "\n",
    "s3 = 30\n",
    "p3 = [13, 27, 41, 59, 28, 33, 39, 19, 52, 48, 55, 79]\n",
    "result3 = highest_possible_score(s3, p3)\n",
    "print(\"Output 3:\", result3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the function is better in terms of optimization. The reason is that it initializes the memo dictionary inside the function. This means that a new memo dictionary is created every time the function is called, which prevents the dictionary from growing indefinitely and using up all the available memory.\n",
    "\n",
    "In the first version of the function, the memo dictionary is initialized in the function’s parameter list. This means that the same memo dictionary is used for all calls to the function, even across different top-level calls. This can cause the dictionary to grow very large if the function is called many times, which can lead to high memory usage.\n",
    "\n",
    "So, in terms of memory optimization, the second version of the function is better. However, in terms of time complexity, both versions of the function have the same time complexity of $O(n^2 \\cdot m)$, where n is the number of elements in the marks list and m is the maximum value in the marks list. This is because the core logic of the function is the same in both versions.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
